"""
CHAIN SYSTEM - Ana Chatbot Sistemi
==================================

Bu dosya LangChain chain yapÄ±sÄ± ile tÃ¼m sistemleri koordine eder.
- Chain-based akÄ±ÅŸ yÃ¶nlendirmesi
- RAG, Animal, Emotion sistemlerini chain olarak Ã§aÄŸÄ±rma
- Web arayÃ¼zÃ¼ yÃ¶netimi
"""

import os
import re
import html
from typing import Any, Dict
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from pathlib import Path
from dotenv import load_dotenv

# LangChain imports
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import BaseOutputParser
from langchain.memory import ConversationSummaryBufferMemory

# Sistem modÃ¼llerini import et
from emotion_system import EmotionChatbot
from statistic_system import StatisticSystem
from animal_system import route_animals, _animal_emoji
from rag_service import rag_service

load_dotenv()

app = FastAPI(title="CHAIN SYSTEM - AkÄ±llÄ± Chatbot Sistemi", version="3.0.0")

# LangChain LLM instance - Fallback mekanizmasÄ± ile
def get_llm():
    """OpenAI API geÃ§ersizse Gemini'yi kullan"""
    try:
        # OpenAI API'yi test et
        print("[LLM] OpenAI API test ediliyor...")
        test_llm = OpenAI(temperature=0.1, max_tokens=1000, request_timeout=15)
        # Basit bir test Ã§aÄŸrÄ±sÄ± yap
        test_result = test_llm.invoke("test")
        print(f"[LLM] OpenAI test sonucu: {test_result}")
        print("[LLM] OpenAI API kullanÄ±lÄ±yor")
        return test_llm
    except Exception as e:
        print(f"[LLM] OpenAI API hatasÄ±: {e}")
        try:
            # Gemini API'yi test et - sadece API key ile
            print("[LLM] Gemini API test ediliyor...")
            import google.generativeai as genai
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise Exception("GEMINI_API_KEY bulunamadÄ±")
            genai.configure(api_key=api_key)
            
            # LangChain olmadan direkt Gemini kullan
            model = genai.GenerativeModel('gemini-2.5-flash')
            # Test Ã§aÄŸrÄ±sÄ± yap
            response = model.generate_content("test")
            print(f"[LLM] Gemini test sonucu: {response.text}")
            
            # LangChain wrapper oluÅŸtur - Google Cloud credentials olmadan
            from langchain_google_genai import ChatGoogleGenerativeAI
            gemini_llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                temperature=0.1,
                max_tokens=1000,
                request_timeout=15,
                google_api_key=api_key
            )
            print("[LLM] Gemini API kullanÄ±lÄ±yor")
            return gemini_llm
        except Exception as gemini_error:
            print(f"[LLM] Gemini API hatasÄ±: {gemini_error}")
            raise Exception(f"API hatasÄ± - OpenAI: {e}, Gemini: {gemini_error}")

llm = get_llm()

# =============================================================================
# CONVERSATIONSUMMARYBUFFERMEMORY - GLOBAL MEMORY SÄ°STEMÄ°
# =============================================================================
# Hibrit yaklaÅŸÄ±m: uzun konuÅŸmalarÄ± Ã¶zetler, son mesajlarÄ± hatÄ±rlar
# Token limiti ile maliyet kontrolÃ¼ saÄŸlar
# TÃ¼m chain'ler bu memory sistemi ile konuÅŸma geÃ§miÅŸini paylaÅŸÄ±r
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=200,  # 200 token limit - maliyet kontrolÃ¼ iÃ§in
    memory_key="chat_history",  # Memory anahtarÄ± - chain'lerde otomatik kullanÄ±lÄ±r
    return_messages=True  # Mesaj formatÄ±nda dÃ¶ndÃ¼r - LangChain uyumluluÄŸu iÃ§in
)

# Global chatbot instance
chatbot_instance: EmotionChatbot | None = None

# =============================================================================
# Ã–ZETLEYÄ°CÄ° (SUMMARIZER) - Uzun kullanÄ±cÄ± mesajlarÄ±nÄ± kÄ±saltmak iÃ§in
# =============================================================================
# T5-small ile transformers pipeline kullanÄ±lÄ±r. Lazy-init yapÄ±lÄ±r ve tek instance
# tutulur. KullanÄ±cÄ± mesajÄ± yaklaÅŸÄ±k 200 token'i aÅŸarsa tetiklenir.
_summarizer_pipeline = None  # Lazy init: ilk kullanÄ±mda yÃ¼klenir

def _get_device_id() -> int:
    """Transformers pipeline iÃ§in cihaz kimliÄŸini dÃ¶ndÃ¼rÃ¼r.
    - CUDA (GPU) mevcutsa 0, deÄŸilse -1 (CPU)
    """
    try:
        import torch  # Yerinde import: baÄŸÄ±mlÄ±lÄ±k yoksa hata kontrollÃ¼ yakalanÄ±r
        return 0 if getattr(torch, "cuda", None) and torch.cuda.is_available() else -1
    except Exception:
        # Torch yoksa CPU kullan
        return -1


def _get_summarizer():
    """T5-small summarization pipeline'Ä±nÄ± dÃ¶ndÃ¼rÃ¼r (lazy-init)."""
    global _summarizer_pipeline
    if _summarizer_pipeline is not None:
        return _summarizer_pipeline
    try:
        # Transformers'Ä± sadece gerektiÄŸinde yÃ¼kle
        from transformers import pipeline  # type: ignore
        device_id = _get_device_id()
        _summarizer_pipeline = pipeline(
            "summarization",
            model="t5-small",
            device=device_id,
        )
        return _summarizer_pipeline
    except Exception as e:
        # Ã–zetleyici yÃ¼klenemezse None dÃ¶ndÃ¼r ve akÄ±ÅŸÄ± engelleme
        print(f"[SUMMARIZER] YÃ¼kleme hatasÄ±: {e}")
        return None


def _summarize_text_if_needed(text: str, estimated_tokens: int, token_threshold: int = 200) -> str:
    """Mesaj token tahmini eÅŸik deÄŸerini aÅŸÄ±yorsa metni kÄ±saltÄ±r.

    GÃ¼venlik/saÄŸlamlÄ±k notlarÄ±:
    - Transformers baÄŸÄ±mlÄ±lÄ±ÄŸÄ± yoksa veya model yÃ¼klenemezse orijinal metni dÃ¶ndÃ¼rÃ¼r
    - T5 iÃ§in "summarize:" prefix'i kullanÄ±lÄ±r; TÃ¼rkÃ§e giriÅŸlerde de Ã§alÄ±ÅŸÄ±r
    - max_new_tokens/min_new_tokens, girdinin uzunluÄŸuna gÃ¶re Ã¶lÃ§eklenir
    """
    try:
        if estimated_tokens <= token_threshold:
            return text

        summarizer = _get_summarizer()
        if summarizer is None:
            return text

        # T5 Ã¶zetleme: Ä°ngilizce Ã¶n-ek; TÃ¼rkÃ§e iÃ§in de kabul edilebilir
        prefixed = "summarize: " + text

        # Tokenizer varsa giriÅŸ uzunluÄŸuna gÃ¶re dinamik ayar yap
        try:
            input_len = len(summarizer.tokenizer(prefixed)["input_ids"])  # type: ignore[attr-defined]
        except Exception:
            # Tokenizer'a eriÅŸilemezse kaba tahmin ile Ã§alÄ±ÅŸ
            input_len = max(60, len(prefixed) // 4)

        # Ã‡Ä±kÄ±ÅŸ uzunluÄŸu: giriÅŸin ~%30-50'si; alt/Ã¼st sÄ±nÄ±rlar gÃ¼venlik iÃ§in
        new_tokens = max(32, min(160, int(input_len * 0.4)))
        min_new = max(16, int(new_tokens * 0.4))

        result = summarizer(
            prefixed,
            max_new_tokens=new_tokens,
            min_new_tokens=min_new,
            do_sample=False,
        )
        summary = ""
        try:
            summary = (result[0] or {}).get("summary_text", "").strip()
        except Exception:
            summary = ""

        # BoÅŸ dÃ¶nerse orijinal metni koru
        if not summary:
            return text

        # Konsola kÄ±saltÄ±lmÄ±ÅŸ Ã§Ä±ktÄ±yÄ± yaz (istenen gereksinim)
        print(f"[SUMMARIZER] KÄ±saltÄ±lmÄ±ÅŸ metin: {summary}")
        return summary
    except Exception as e:
        # Her tÃ¼rlÃ¼ hata durumunda orijinal metni dÃ¶ndÃ¼r
        print(f"[SUMMARIZER] Ã‡alÄ±ÅŸma hatasÄ±: {e}")
        return text

# RAG modelini asenkron olarak Ã¶nceden yÃ¼kle
print("[CHAIN SYSTEM] RAG modeli asenkron olarak yÃ¼kleniyor...")
rag_service.preload_model_async()

# GÃ¼venlik sabitleri
MAX_MESSAGE_LENGTH = 2000  # Maksimum mesaj uzunluÄŸu
MAX_TOKENS_PER_REQUEST = 1000  # Maksimum token sayÄ±sÄ±
DANGEROUS_PATTERNS = [
    r'<script[^>]*>.*?</script>',  # Script injection
    r'javascript:',  # JavaScript URL
    r'data:text/html',  # Data URL
    r'vbscript:',  # VBScript
    r'on\w+\s*=',  # Event handlers
    r'<iframe[^>]*>',  # Iframe injection
    r'<object[^>]*>',  # Object injection
    r'<embed[^>]*>',  # Embed injection
    r'<link[^>]*>',  # Link injection
    r'<meta[^>]*>',  # Meta injection
]

# RAG kaynaklarÄ± (UI id'leri sabit: pdf-python/anayasa/clean)
RAG_SOURCES = {
    "cat_care.pdf": {"id": "pdf-python", "emoji": "ğŸ±", "alias": "cat"},
    "parrot_care.pdf": {"id": "pdf-anayasa", "emoji": "ğŸ¦œ", "alias": "parrot"},
    "rabbit_care.pdf": {"id": "pdf-clean", "emoji": "ğŸ°", "alias": "rabbit"},
}

# Static files (CSS/JS)
STATIC_DIR = Path(__file__).parent / "static"
try:
    STATIC_DIR.mkdir(parents=True, exist_ok=True)
except Exception:
    pass
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")


class FlowDecisionParser(BaseOutputParser):
    """AkÄ±ÅŸ kararÄ± parser'Ä± - LLM Ã§Ä±ktÄ±sÄ±nÄ± temizler"""
    
    def parse(self, text: str) -> str:
        """LLM Ã§Ä±ktÄ±sÄ±nÄ± temizleyip akÄ±ÅŸ kararÄ±nÄ± dÃ¶ndÃ¼rÃ¼r"""
        text = text.strip().upper()
        valid_flows = ["ANIMAL", "RAG", "EMOTION", "STATS", "HELP"]
        
        for flow in valid_flows:
            if flow in text:
                return flow
        
        return "HELP"  # VarsayÄ±lan fallback - yardÄ±m mesajÄ±


def _sanitize_input(text: str) -> str:
    """GÃ¼venli input sanitization - injection saldÄ±rÄ±larÄ±nÄ± Ã¶nler"""
    if not text:
        return ""
    
    # HTML escape
    text = html.escape(text, quote=True)
    
    # Tehlikeli pattern'leri kontrol et
    for pattern in DANGEROUS_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            print(f"[SECURITY] Tehlikeli pattern tespit edildi: {pattern}")
            return "[GÃ¼venlik nedeniyle mesaj filtrelendi]"
    
    # Fazla boÅŸluklarÄ± temizle
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def _validate_message_length(text: str) -> bool:
    """Mesaj uzunluÄŸunu kontrol eder"""
    return len(text) <= MAX_MESSAGE_LENGTH


def _estimate_tokens(text: str) -> int:
    """YaklaÅŸÄ±k token sayÄ±sÄ±nÄ± hesaplar (TÃ¼rkÃ§e iÃ§in)"""
    # TÃ¼rkÃ§e iÃ§in yaklaÅŸÄ±k hesaplama: 1 token â‰ˆ 4 karakter
    return len(text) // 4


# =============================================================================
# CHAIN SYSTEM - LangChain Chain YapÄ±larÄ±
# =============================================================================

def create_flow_decision_chain():
    """AkÄ±ÅŸ kararÄ± chain'i oluÅŸturur - ConversationSummaryBufferMemory ile"""
    # Memory sistemi ile akÄ±ÅŸ kararÄ± - Ã¶nceki konuÅŸma baÄŸlamÄ± otomatik eklenir
    flow_prompt = PromptTemplate(
        input_variables=["input"],
        template="""KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± analiz et ve ÅŸu akÄ±ÅŸlardan birini seÃ§:

Ã–NEMLÄ° KURALLAR:
1. EÄŸer kullanÄ±cÄ± BÄ°LGÄ° istiyorsa (hayvan bakÄ±mÄ±, beslenme, barÄ±nma, saÄŸlÄ±k, eÄŸitim, bakÄ±m Ã¶nerileri) â†’ RAG
2. EÄŸer kullanÄ±cÄ± HAYVAN istiyorsa (kÃ¶pek, kedi, tilki, Ã¶rdek fotoÄŸraf/bilgi) â†’ ANIMAL  
3. EÄŸer kullanÄ±cÄ± SOHBET/DUYGU istiyorsa (merhaba, nasÄ±lsÄ±n, Ã¼zgÃ¼nÃ¼m, mutluyum) â†’ EMOTION
4. EÄŸer kullanÄ±cÄ± Ä°STATÄ°STÄ°K/Ã–ZET istiyorsa ("kaÃ§ kez/defa", "istatistik", "Ã¶zet", belirli duygu istatistiÄŸi, bugÃ¼n/bugÃ¼ne ait sayÄ±m) â†’ STATS
5. EÄŸer kullanÄ±cÄ± hiÃ§bir Ã¶zelliÄŸi Ã§aÄŸÄ±rmÄ±yorsa (genel sorular, yardÄ±m, ne yapabilirsin) â†’ HELP

AkÄ±ÅŸlar:
- ANIMAL: KÃ¶pek, kedi, tilki, Ã¶rdek fotoÄŸraf/bilgi isteÄŸi
- RAG: Kedi/PapaÄŸan/TavÅŸan bakÄ±mÄ±, beslenme, barÄ±nma, saÄŸlÄ±k, eÄŸitim, bakÄ±m rutinleri
- EMOTION: Duygu analizi, sohbet, normal konuÅŸma
- STATS: Duygu istatistikleri (today/all + isteÄŸe baÄŸlÄ± duygu filtresi)
- HELP: YardÄ±m, ne yapabilirsin, genel bilgi istekleri

KullanÄ±cÄ± MesajÄ±: {input}

Sadece ÅŸu yanÄ±tlardan birini ver: ANIMAL, RAG, EMOTION, STATS, HELP"""
    )
    
    def flow_processor(input_data):
        """Flow decision iÅŸleyicisi - Gemini ve OpenAI Ã§Ä±ktÄ±larÄ±nÄ± normalize eder"""
        result = (flow_prompt | llm).invoke(input_data)
        
        # Ham cevabÄ± konsola yazdÄ±r
        print(f"[FLOW DEBUG] Ham result tipi: {type(result)}")
        print(f"[FLOW DEBUG] Ham result: {result}")
        
        # Gemini ve OpenAI Ã§Ä±ktÄ±larÄ±nÄ± normalize et
        if hasattr(result, 'content'):
            # LangChain response objesi
            text = result.content
            print(f"[FLOW DEBUG] Content: {text}")
        elif isinstance(result, str):
            # String Ã§Ä±ktÄ±
            text = result
            print(f"[FLOW DEBUG] String: {text}")
        else:
            # DiÄŸer durumlar iÃ§in string'e Ã§evir
            text = str(result)
            print(f"[FLOW DEBUG] String'e Ã§evriliyor: {text}")
        
        # FlowDecisionParser'Ä± kullan
        parser = FlowDecisionParser()
        parsed_result = parser.parse(text)
        print(f"[FLOW DEBUG] Parsed result: {parsed_result}")
        return parsed_result
    
    return flow_processor


def create_rag_chain():
    """RAG chain'i oluÅŸturur - ConversationSummaryBufferMemory ile"""
    # Memory ile kullanÄ±rken sadece tek input variable kullan - chat_history otomatik eklenir
    # Context bilgisi prompt'a dahil edilir, memory sistemi konuÅŸma geÃ§miÅŸini yÃ¶netir
    rag_prompt = PromptTemplate(
        input_variables=["input"],
        template="""Sen bir hayvan bakÄ±mÄ± bilgi asistanÄ±sÄ±n. Verilen BAÄLAM (PDF parÃ§alarÄ±) Ã¼zerinden
kullanÄ±cÄ±nÄ±n sorusunu YALNIZCA baÄŸlama dayanarak yanÄ±tla. TÃ¼rkÃ§e, kÄ±sa, net ve uygulanabilir yaz.

Kesin kurallar:
1) DoÄŸrudan yanÄ±t ver; bÃ¶lÃ¼m/baÅŸlÄ±k/"git oku" tarzÄ± yÃ¶nlendirmeler yapma.
2) BaÄŸlamdaki bilgileri Ã¶zlÃ¼ maddeler halinde veya kÄ±sa paragraflarla aktar.
3) Kaynak ve alÄ±ntÄ± isimlerini yazma; sadece iÃ§erik ver.
4) BaÄŸlam yeterli deÄŸilse bunu aÃ§Ä±kÃ§a sÃ¶yle: "BaÄŸlamda yeterli bilgi yok."
5) JSON Ã¼retme; normal metin ver. Maksimum 5 cÃ¼mle.

SORU VE BAÄLAM: {input}

YANIT:"""
    )
    
    def rag_processor(input_data):
        """RAG iÅŸleyicisi - Gemini ve OpenAI Ã§Ä±ktÄ±larÄ±nÄ± normalize eder"""
        result = (rag_prompt | llm).invoke(input_data)
        
        # Ham cevabÄ± konsola yazdÄ±r
        print(f"[RAG DEBUG] Ham result tipi: {type(result)}")
        print(f"[RAG DEBUG] Ham result: {result}")
        
        # Gemini ve OpenAI Ã§Ä±ktÄ±larÄ±nÄ± normalize et
        if hasattr(result, 'content'):
            # LangChain response objesi
            print(f"[RAG DEBUG] Content: {result.content}")
            return result.content
        elif isinstance(result, str):
            # String Ã§Ä±ktÄ±
            print(f"[RAG DEBUG] String: {result}")
            return result
        else:
            # DiÄŸer durumlar iÃ§in string'e Ã§evir
            print(f"[RAG DEBUG] String'e Ã§evriliyor: {str(result)}")
            return str(result)
    
    return rag_processor


def create_animal_chain():
    """Animal chain'i oluÅŸturur - API Ã§aÄŸrÄ±sÄ± yapar - ConversationSummaryBufferMemory ile"""
    def animal_processor(user_message: str) -> Dict[str, Any]:
        """Hayvan API'sini Ã§aÄŸÄ±rÄ±r ve sonucu dÃ¶ndÃ¼rÃ¼r - memory sistemi ile timeout handling"""
        # Hayvan API'leri iÃ§in timeout ve hata yÃ¶netimi
        # Memory sistemi ile konuÅŸma geÃ§miÅŸi otomatik olarak yÃ¶netiliyor
        try:
            print("[ANIMAL CHAIN] Hayvan API'si Ã§aÄŸrÄ±lÄ±yor...")
            # OpenAI client oluÅŸtur - route_animals client bekliyor
            from openai import OpenAI
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            animal_result = route_animals(user_message, client)
            
            if animal_result:
                animal = str(animal_result.get("animal", ""))
                out: Dict[str, Any] = {
                    "animal": animal,
                    "type": animal_result.get("type"),
                    "animal_emoji": _animal_emoji(animal),
                }
                if animal_result.get("type") == "image":
                    out["image_url"] = animal_result.get("image_url")
                    out["response"] = f"{_animal_emoji(animal)} {animal.capitalize()} fotoÄŸrafÄ± hazÄ±r."
                else:
                    out["response"] = animal_result.get("text", "")
                
                # Memory'ye animal yanÄ±tÄ±nÄ± kaydet
                memory.save_context(
                    {"input": user_message},
                    {"output": out["response"]}
                )
                
                print(f"[ANIMAL CHAIN] BaÅŸarÄ±lÄ±: {animal}")
                return out
            
            # Hayvan bulunamadÄ± durumu iÃ§in de memory'ye kaydet
            error_response = "Hayvan bulunamadÄ±."
            memory.save_context(
                {"input": user_message},
                {"output": error_response}
            )
            print("[ANIMAL CHAIN] Hayvan bulunamadÄ±")
            return {"response": error_response}
            
        except Exception as e:
            print(f"[ANIMAL CHAIN] Hata: {e}")
            # Timeout veya API hatasÄ± durumunda fallback yanÄ±t
            error_response = "Hayvan API'si ÅŸu anda kullanÄ±lamÄ±yor. LÃ¼tfen daha sonra tekrar deneyin."
            memory.save_context(
                {"input": user_message},
                {"output": error_response}
            )
            return {"response": error_response}
    
    return animal_processor


def create_emotion_chain():
    """Emotion chain'i oluÅŸturur - ConversationSummaryBufferMemory ile"""
    def emotion_processor(user_message: str) -> Dict[str, Any]:
        """Duygu analizi yapar - memory sistemi ile"""
        # Emotion sistemi iÃ§in OpenAI client oluÅŸturma
        # Memory sistemi ile konuÅŸma geÃ§miÅŸi otomatik olarak yÃ¶netiliyor
        global chatbot_instance
        if chatbot_instance is None:
            # Fallback mekanizmasÄ± ile client oluÅŸtur
            try:
                from openai import OpenAI
                client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
                # Test Ã§aÄŸrÄ±sÄ± yap
                client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": "test"}],
                    max_tokens=1
                )
                chatbot_instance = EmotionChatbot(client)
                print("[EMOTION] OpenAI API kullanÄ±lÄ±yor")
            except Exception as e:
                print(f"[EMOTION] OpenAI API hatasÄ±: {e}")
                # Gemini kullan
                chatbot_instance = EmotionChatbot()  # client=None, Gemini kullanacak
                print("[EMOTION] Gemini API kullanÄ±lÄ±yor")
        
        # Memory sistemi ile Ã¶nceki konuÅŸma geÃ§miÅŸi otomatik olarak yÃ¶netiliyor
        
        result = chatbot_instance.chat(user_message)
        stats = {
            "requests": chatbot_instance.stats["requests"],
            "last_request_at": chatbot_instance.stats["last_request_at"],
        }
        
        # Memory'ye yeni konuÅŸmayÄ± kaydet
        memory.save_context(
            {"input": user_message},
            {"output": result.get("response", "")}
        )
        
        out = {"response": result.get("response", ""), "stats": stats}
        if "first_emoji" in result:
            out["first_emoji"] = result["first_emoji"]
        if "second_emoji" in result:
            out["second_emoji"] = result["second_emoji"]
        if "request_debug" in result:
            out["request_debug"] = result["request_debug"]
        return out
    
    return emotion_processor


def create_stats_chain():
    """Stats chain'i oluÅŸturur - data/ dosyalarÄ±ndan hesaplar"""
    stats_system = StatisticSystem()

    def stats_processor(user_message: str) -> Dict[str, Any]:
        try:
            result = stats_system.answer(user_message)
            # Memory'ye kaydet
            memory.save_context({"input": user_message}, {"output": result.get("response", "")})
            return result
        except Exception as e:
            err = f"Ä°statistik sistemi hatasÄ±: {e}"
            memory.save_context({"input": user_message}, {"output": err})
            return {"response": err}

    return stats_processor

# =============================================================================
# CHAIN SYSTEM - Ana Ä°ÅŸlem Zinciri
# =============================================================================

def create_main_processing_chain():
    """Ana iÅŸlem zinciri oluÅŸturur"""
    
    # Alt chain'leri oluÅŸtur
    flow_decision_chain = create_flow_decision_chain()
    rag_chain = create_rag_chain()
    animal_processor = create_animal_chain()
    emotion_processor = create_emotion_chain()
    stats_processor = create_stats_chain()
    
    def process_message(user_message: str) -> Dict[str, Any]:
        """Ana mesaj iÅŸleme fonksiyonu - ConversationSummaryBufferMemory ile"""
        try:
            print("[CHAIN SYSTEM] AÅAMA 1: AkÄ±ÅŸ kararÄ± alÄ±nÄ±yor...")
            
            # Memory sistemi aktif - ConversationSummaryBufferMemory ile konuÅŸma geÃ§miÅŸi yÃ¶netiliyor
            
            # AÅAMA 1: AkÄ±ÅŸ kararÄ±
            flow_decision = flow_decision_chain({"input": user_message})
            print(f"[CHAIN SYSTEM] AkÄ±ÅŸ kararÄ±: {flow_decision}")
            
            # AÅAMA 2: SeÃ§ilen akÄ±ÅŸa gÃ¶re iÅŸleme
            if flow_decision == "RAG":
                print("[CHAIN SYSTEM] AÅAMA 2: RAG akÄ±ÅŸÄ± Ã§alÄ±ÅŸÄ±yor...")
                rag_result = _process_rag_flow(user_message, rag_chain)
                if rag_result is None:
                    print("[CHAIN SYSTEM] RAG sonucu None, HELP akÄ±ÅŸÄ±na yÃ¶nlendiriliyor...")
                    help_result = _process_help_flow(user_message)
                    help_result["flow_type"] = "HELP"
                    return help_result
                rag_result["flow_type"] = "RAG"
                return rag_result
            elif flow_decision == "ANIMAL":
                print("[CHAIN SYSTEM] AÅAMA 2: Animal akÄ±ÅŸÄ± Ã§alÄ±ÅŸÄ±yor...")
                animal_result = animal_processor(user_message)
                animal_result["flow_type"] = "ANIMAL"
                return animal_result
            elif flow_decision == "EMOTION":
                print("[CHAIN SYSTEM] AÅAMA 2: Emotion akÄ±ÅŸÄ± Ã§alÄ±ÅŸÄ±yor...")
                emotion_result = emotion_processor(user_message)
                emotion_result["flow_type"] = "EMOTION"
                return emotion_result
            elif flow_decision == "STATS":
                print("[CHAIN SYSTEM] AÅAMA 2: Stats akÄ±ÅŸÄ± Ã§alÄ±ÅŸÄ±yor...")
                stats_result = stats_processor(user_message)
                stats_result["flow_type"] = "STATS"
                return stats_result
            elif flow_decision == "HELP":
                print("[CHAIN SYSTEM] AÅAMA 2: Help akÄ±ÅŸÄ± Ã§alÄ±ÅŸÄ±yor...")
                result = _process_help_flow(user_message)
                result["flow_type"] = "HELP"
                print(f"[CHAIN SYSTEM] Help result: {result}")
                return result
            else:
                print("[CHAIN SYSTEM] Fallback: Help akÄ±ÅŸÄ± Ã§alÄ±ÅŸÄ±yor...")
                result = _process_help_flow(user_message)
                result["flow_type"] = "HELP"
                print(f"[CHAIN SYSTEM] Fallback result: {result}")
                return result
                
        except Exception as e:
            print(f"[CHAIN SYSTEM] Hata: {e}")
            return {"error": str(e)}
    
    return process_message


def _process_rag_flow(user_message: str, rag_chain) -> Dict[str, Any] | None:
    """RAG akÄ±ÅŸÄ±nÄ± iÅŸler - PDF'lerden bilgi Ã§eker"""
    t = user_message.lower()
    
    # Heuristic: explicit source keywords (hayvan bakÄ±m)
    if ("kedi" in t or "cat" in t):
        source = "cat_care.pdf"
    elif ("papaÄŸan" in t or "parrot" in t or "kuÅŸ" in t):
        source = "parrot_care.pdf"
    elif ("tavÅŸan" in t or "rabbit" in t):
        source = "rabbit_care.pdf"
    else:
        # LLM RAG seÃ§tiyse anahtar kelime kontrolÃ¼ yapmadan genel retrieval dene
        chunks = rag_service.retrieve_top(user_message, top_k=6)
        if not chunks:
            print("[RAG] RAG'de ilgili bilgi bulunamadÄ±")
            return None
        context = "\n\n".join([c.get("text", "") for c in chunks])
        sources = list({(c.get("metadata", {}) or {}).get("source", "?") for c in chunks})
        
        # RAG chain ile iÅŸle - context'i prompt'a dahil et
        combined_input = f"BAÄLAM:\n{context}\n\nSORU: {user_message}"
        result = rag_chain({"input": combined_input})
        
        # Memory'ye RAG yanÄ±tÄ±nÄ± kaydet
        memory.save_context(
            {"input": user_message},
            {"output": result if isinstance(result, str) else str(result)}
        )
        
        # Pick first known source for UI hint
        lit = None
        for s in sources:
            if s in RAG_SOURCES:
                lit = s
                break
        ui = RAG_SOURCES.get(lit or "", None)
        return {
            "rag": True,
            "response": result if isinstance(result, str) else str(result),
            "rag_source": ui.get("id") if ui else None,
            "rag_emoji": ui.get("emoji") if ui else None,
        }

    # Source-filtered retrieval
    chunks = rag_service.retrieve_by_source(user_message, source_filename=source, top_k=6)
    if not chunks:
        return None
    context = "\n\n".join([c.get("text", "") for c in chunks])
    
    # RAG chain ile iÅŸle - context'i prompt'a dahil et
    combined_input = f"BAÄLAM:\n{context}\n\nSORU: {user_message}"
    result = rag_chain({"input": combined_input})
    
    # Memory'ye RAG yanÄ±tÄ±nÄ± kaydet
    memory.save_context(
        {"input": user_message},
        {"output": result if isinstance(result, str) else str(result)}
    )
    
    ui = RAG_SOURCES.get(source)
    return {
        "rag": True,
        "response": result if isinstance(result, str) else str(result),
        "rag_source": ui.get("id"),
        "rag_emoji": ui.get("emoji"),
    }


def _process_help_flow(user_message: str) -> Dict[str, Any]:
    """Help akÄ±ÅŸÄ±nÄ± iÅŸler - kullanÄ±cÄ±ya yÃ¶nlendirici mesaj verir"""
    help_message = """ğŸ¤– Merhaba! Ben akÄ±llÄ± bir chatbot'um ve size ÅŸu Ã¶zelliklerle yardÄ±mcÄ± olabilirim:

ğŸ“š **BÄ°LGÄ° SÄ°STEMÄ° (RAG)**: 
â€¢ Kedi / PapaÄŸan / TavÅŸan bakÄ±mÄ± (beslenme, barÄ±nma, saÄŸlÄ±k, eÄŸitim)
â€¢ "Kedi yavrusu nasÄ±l beslenir?", "PapaÄŸan kafes bakÄ±mÄ± nasÄ±l yapÄ±lÄ±r?", "TavÅŸan tÄ±rnak kesimi nasÄ±l yapÄ±lÄ±r?"

ğŸ¶ **HAYVAN SÄ°STEMÄ°**:
â€¢ KÃ¶pek, kedi, tilki, Ã¶rdek fotoÄŸraf ve bilgileri
â€¢ "kÃ¶pek fotoÄŸrafÄ± ver", "kedi bilgisi ver" gibi istekler

ğŸ’­ **DUYGU ANALÄ°ZÄ°**:
â€¢ DuygularÄ±nÄ±zÄ± analiz eder ve size uygun yanÄ±tlar verir
â€¢ "BugÃ¼n Ã§ok mutluyum", "ÃœzgÃ¼n hissediyorum" gibi mesajlar

ğŸ¯ **KULLANIM**: Ekranda gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z kutucuklarÄ± kullanarak veya yukarÄ±daki Ã¶rnekler gibi mesajlar gÃ¶ndererek bu chatbot'u kullanabilirsiniz!"""
    
    # Memory'ye help yanÄ±tÄ±nÄ± kaydet
    memory.save_context(
        {"input": user_message},
        {"output": help_message}
    )
    
    return {
        "help": True,
        "response": help_message
    }


# Ana chain'i oluÅŸtur
main_chain = create_main_processing_chain()


# =============================================================================
# FASTAPI ENDPOINTS
# =============================================================================

@app.get("/", response_class=HTMLResponse)
def index() -> HTMLResponse:
    """Ana sayfa HTML'ini dÃ¶ndÃ¼rÃ¼r"""
    template_path = Path(__file__).parent / "templates" / "index.html"
    try:
        html = template_path.read_text(encoding="utf-8")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HTML yÃ¼klenemedi: {e}")
    return HTMLResponse(content=html)


@app.post("/chat")
def chat(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Ana chat endpoint'i - CHAIN SYSTEM ile akÄ±ÅŸ yÃ¶nlendirmesi yapar"""
    user_message = str(payload.get("message", "")).strip()
    
    # GÃ¼venlik kontrolleri
    if not user_message:
        return {"error": "Mesaj boÅŸ olamaz"}
    
    # Mesaj uzunluk kontrolÃ¼
    if not _validate_message_length(user_message):
        return {"error": f"Mesaj Ã§ok uzun. Maksimum {MAX_MESSAGE_LENGTH} karakter olabilir."}
    
    # Input sanitization
    user_message = _sanitize_input(user_message)
    if user_message == "[GÃ¼venlik nedeniyle mesaj filtrelendi]":
        return {"error": "GÃ¼venlik nedeniyle mesaj filtrelendi"}
    
    # Token kontrolÃ¼
    estimated_tokens = _estimate_tokens(user_message)
    # 200+ token ise Ã¶nce Ã¶zetlemeyi dene (kaba tahmin Ã¼zerinden)
    user_message = _summarize_text_if_needed(user_message, estimated_tokens, token_threshold=200)
    # Ã–zet sonrasÄ± yeniden tahmini token sayÄ±sÄ± al (sert limit iÃ§in paylaÅŸÄ±mcÄ± davranÄ±ÅŸ)
    estimated_tokens = _estimate_tokens(user_message)
    if estimated_tokens > MAX_TOKENS_PER_REQUEST:
        return {"error": f"Ã‡ok fazla token. Maksimum {MAX_TOKENS_PER_REQUEST} token olabilir."}

    try:
        # CHAIN SYSTEM ile mesaj iÅŸleme
        print("[CHAIN SYSTEM] Mesaj iÅŸleniyor...")
        result = main_chain(user_message)
        
        # Result'u kontrol et ve hata varsa dÃ¼zelt
        if isinstance(result, dict) and "error" in result:
            print(f"[CHAIN SYSTEM] Hata tespit edildi: {result['error']}")
            return {"error": result["error"]}
        
        # Result'un geÃ§erli olduÄŸundan emin ol
        if not isinstance(result, dict):
            print(f"[CHAIN SYSTEM] GeÃ§ersiz result tipi: {type(result)}")
            return {"error": "GeÃ§ersiz response formatÄ±"}
            
        print(f"[CHAIN SYSTEM] BaÅŸarÄ±lÄ± response: {result}")
        return result

    except HTTPException as e:
        print(f"[CHAIN SYSTEM] HTTPException: {e.detail}")
        return {"error": e.detail}
    except Exception as e:
        print(f"[CHAIN SYSTEM] Exception: {str(e)}")
        return {"error": f"Sunucu hatasÄ±: {str(e)}"}


# Ã‡alÄ±ÅŸtÄ±rma:
# uvicorn api_web_chatbot:app --host 0.0.0.0 --port 8000 --reload

if __name__ == "__main__":
    """Script doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda Uvicorn ile sunucuyu baÅŸlatÄ±r.
    - Host: 0.0.0.0
    - Port: 8000
    - Reload: True (geliÅŸtirme iÃ§in otomatik yeniden yÃ¼kleme)
    """
    try:
        # Uvicorn'u sadece ihtiyaÃ§ olduÄŸunda import et (dinamik import)
        import uvicorn  # type: ignore
        # ModÃ¼l:app ÅŸeklinde string kullanmak, reload modunda saÄŸlÄ±klÄ± yeniden yÃ¼kleme saÄŸlar
        uvicorn.run("api_web_chatbot:app", host="0.0.0.0", port=8000, reload=True)
    except Exception as e:
        # Ã‡alÄ±ÅŸma zamanÄ±nda oluÅŸabilecek hatalarÄ± kullanÄ±cÄ±ya bildir
        print(f"[SERVER] Uvicorn baÅŸlatma hatasÄ±: {e}")