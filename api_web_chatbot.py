"""
Ana Chatbot Sistemi
==================

Bu dosya t√ºm sistemleri koordine eden ana y√∂nlendirme sistemidir.
- LLM ile akƒ±≈ü y√∂nlendirmesi
- RAG, Animal, Emotion sistemlerini √ßaƒüƒ±rma
- Web aray√ºz√º y√∂netimi
"""

import os
import re
import html
from typing import Any, Dict
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

# Sistem mod√ºllerini import et
from emotion_system import EmotionChatbot
from animal_system import route_animals, _animal_emoji
from rag_service import rag_service

load_dotenv()

app = FastAPI(title="Akƒ±llƒ± Chatbot Sistemi", version="2.0.0")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Global chatbot instance
chatbot_instance: EmotionChatbot | None = None

# RAG modelini asenkron olarak √∂nceden y√ºkle
print("[STARTUP] RAG modeli asenkron olarak y√ºkleniyor...")
rag_service.preload_model_async()

# G√ºvenlik sabitleri
MAX_MESSAGE_LENGTH = 2000  # Maksimum mesaj uzunluƒüu
MAX_TOKENS_PER_REQUEST = 1000  # Maksimum token sayƒ±sƒ±
DANGEROUS_PATTERNS = [
    r'<script[^>]*>.*?</script>',  # Script injection
    r'javascript:',  # JavaScript URL
    r'data:text/html',  # Data URL
    r'vbscript:',  # VBScript
    r'on\w+\s*=',  # Event handlers
    r'<iframe[^>]*>',  # Iframe injection
    r'<object[^>]*>',  # Object injection
    r'<embed[^>]*>',  # Embed injection
    r'<link[^>]*>',  # Link injection
    r'<meta[^>]*>',  # Meta injection
]

# RAG kaynaklarƒ±
RAG_SOURCES = {
    "Learning_Python.pdf": {"id": "pdf-python", "emoji": "üêç", "alias": "python"},
    "gerekceli_anayasa.pdf": {"id": "pdf-anayasa", "emoji": "‚öñÔ∏è", "alias": "anayasa"},
    "clean_architecture.pdf": {"id": "pdf-clean", "emoji": "üèóÔ∏è", "alias": "clean"},
}

# Static files (CSS/JS)
STATIC_DIR = Path(__file__).parent / "static"
try:
    STATIC_DIR.mkdir(parents=True, exist_ok=True)
except Exception:
    pass
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")


def _sanitize_input(text: str) -> str:
    """G√ºvenli input sanitization - injection saldƒ±rƒ±larƒ±nƒ± √∂nler"""
    if not text:
        return ""
    
    # HTML escape
    text = html.escape(text, quote=True)
    
    # Tehlikeli pattern'leri kontrol et
    for pattern in DANGEROUS_PATTERNS:
        if re.search(pattern, text, re.IGNORECASE):
            print(f"[SECURITY] Tehlikeli pattern tespit edildi: {pattern}")
            return "[G√ºvenlik nedeniyle mesaj filtrelendi]"
    
    # Fazla bo≈üluklarƒ± temizle
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def _validate_message_length(text: str) -> bool:
    """Mesaj uzunluƒüunu kontrol eder"""
    return len(text) <= MAX_MESSAGE_LENGTH


def _estimate_tokens(text: str) -> int:
    """Yakla≈üƒ±k token sayƒ±sƒ±nƒ± hesaplar (T√ºrk√ße i√ßin)"""
    # T√ºrk√ße i√ßin yakla≈üƒ±k hesaplama: 1 token ‚âà 4 karakter
    return len(text) // 4


def _get_flow_decision(user_message: str) -> str:
    """LLM ile akƒ±≈ü y√∂nlendirmesi yapar (ANIMAL/RAG/EMOTION)"""
    try:
        # Token kontrol√º
        estimated_tokens = _estimate_tokens(user_message)
        if estimated_tokens > MAX_TOKENS_PER_REQUEST:
            print(f"[SECURITY] √áok fazla token: {estimated_tokens}")
            return "EMOTION"  # G√ºvenli fallback
        
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": """Kullanƒ±cƒ±nƒ±n mesajƒ±nƒ± analiz et ve ≈üu akƒ±≈ülardan birini se√ß:

√ñNEMLƒ∞ KURALLAR:
1. Eƒüer kullanƒ±cƒ± Bƒ∞LGƒ∞ istiyorsa (nedir, nasƒ±l, a√ßƒ±kla, tanƒ±m, principle, concept, theory) ‚Üí RAG
2. Eƒüer kullanƒ±cƒ± HAYVAN istiyorsa (k√∂pek, kedi, tilki, √∂rdek fotoƒüraf/bilgi) ‚Üí ANIMAL  
3. Eƒüer kullanƒ±cƒ± SOHBET/DUYGU istiyorsa (merhaba, nasƒ±lsƒ±n, √ºzg√ºn√ºm, mutluyum) ‚Üí EMOTION

Akƒ±≈ülar:
- ANIMAL: K√∂pek, kedi, tilki, √∂rdek fotoƒüraf/bilgi isteƒüi
- RAG: Python, Anayasa, Clean Architecture, teknik terimler, bilgi sorularƒ±, "nedir", "nasƒ±l", "a√ßƒ±kla", "tanƒ±m", "principle", "concept"
- EMOTION: Duygu analizi, sohbet, normal konu≈üma

Sadece ≈üu yanƒ±tlardan birini ver: ANIMAL, RAG, EMOTION"""},
                {"role": "user", "content": user_message}
            ],
            temperature=0.1,
            max_tokens=15
        )
        flow_decision = completion.choices[0].message.content or ""
        return flow_decision.strip()
    except Exception as e:
        print(f"[FLOW] LLM y√∂nlendirme hatasƒ±: {e}")
        return "EMOTION"  # Varsayƒ±lan


def _process_rag_flow(user_message: str) -> Dict[str, Any] | None:
    """RAG akƒ±≈üƒ±nƒ± i≈üler - PDF'lerden bilgi √ßeker"""
    t = user_message.lower()
    # Heuristic: explicit source keywords
    if "anayasa" in t:
        source = "gerekceli_anayasa.pdf"
    elif ("clean architecture" in t or "clean architecture".replace(" ", "_") in t or 
          ("clean" in t and "architecture" in t) or "acyclic" in t or "dependency" in t or 
          "principle" in t or "principles" in t or "dependencies" in t):
        source = "clean_architecture.pdf"
    elif "python" in t:
        source = "Learning_Python.pdf"
    else:
        # If generic question, try general retrieval (no source filter)
        keywords = ["pdf", "belge", "dok√ºman", "√∂zetle", "a√ßƒ±kla", "nedir", "nasƒ±l", "anlat", "tanƒ±m"]
        if not any(k in t for k in keywords):
            return None
        chunks = rag_service.retrieve_top(user_message, top_k=4)
        if not chunks:
            return None
        context = "\n\n".join([c.get("text", "") for c in chunks])
        sources = list({(c.get("metadata", {}) or {}).get("source", "?") for c in chunks})
        prompt = f"BAƒûLAM:\n{context}\n\nSORU: {user_message}\nYANIT:"

        completion = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Sen bir bilgi asistanƒ±sƒ±n. Kullanƒ±cƒ±nƒ±n sorularƒ±nƒ± verilen baƒülam bilgilerini kullanarak yanƒ±tla. T√ºrk√ße, kƒ±sa ve net yanƒ±tlar ver. Baƒülam bilgisini kullan ama gereksiz detay verme. Eƒüer baƒülamda yeterli bilgi yoksa bunu belirt. Yanƒ±tƒ±nƒ± doƒürudan metin olarak ver (JSON formatƒ±nda deƒüil). Maksimum 5 c√ºmle ile yanƒ±tla. √ñzellikle Clean Architecture, Python, Anayasa konularƒ±nda uzmanla≈ümƒ±≈üsƒ±n."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        answer = completion.choices[0].message.content or ""
        # Pick first known source for UI hint
        lit = None
        for s in sources:
            if s in RAG_SOURCES:
                lit = s
                break
        ui = RAG_SOURCES.get(lit or "", None)
        return {
            "rag": True,
            "response": answer,
            "rag_source": ui.get("id") if ui else None,
            "rag_emoji": ui.get("emoji") if ui else None,
        }

    # Source-filtered retrieval
    chunks = rag_service.retrieve_by_source(user_message, source_filename=source, top_k=4)
    if not chunks:
        return None
    context = "\n\n".join([c.get("text", "") for c in chunks])
    prompt = f"BAƒûLAM:\n{context}\n\nSORU: {user_message}\nYANIT:"
    
    completion = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "Sen bir bilgi asistanƒ±sƒ±n. Kullanƒ±cƒ±nƒ±n sorularƒ±nƒ± verilen baƒülam bilgilerini kullanarak yanƒ±tla. T√ºrk√ße, kƒ±sa ve net yanƒ±tlar ver. Baƒülam bilgisini kullan ama gereksiz detay verme. Eƒüer baƒülamda yeterli bilgi yoksa bunu belirt. Yanƒ±tƒ±nƒ± doƒürudan metin olarak ver (JSON formatƒ±nda deƒüil). Maksimum 5 c√ºmle ile yanƒ±tla. √ñzellikle Clean Architecture, Python, Anayasa konularƒ±nda uzmanla≈ümƒ±≈üsƒ±n."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )
    answer = completion.choices[0].message.content or ""
    ui = RAG_SOURCES.get(source)
    return {
        "rag": True,
        "response": answer,
        "rag_source": ui.get("id"),
        "rag_emoji": ui.get("emoji"),
    }


def _process_animal_flow(user_message: str) -> Dict[str, Any] | None:
    """Hayvan akƒ±≈üƒ±nƒ± i≈üler - fotoƒüraf/bilgi getirir"""
    animal_result = route_animals(user_message, client)
    if animal_result:
        animal = str(animal_result.get("animal", ""))
        out: Dict[str, Any] = {
            "animal": animal,
            "type": animal_result.get("type"),
            "animal_emoji": _animal_emoji(animal),
        }
        if animal_result.get("type") == "image":
            out["image_url"] = animal_result.get("image_url")
            out["response"] = f"{_animal_emoji(animal)} {animal.capitalize()} fotoƒürafƒ± hazƒ±r."
        else:
            out["response"] = animal_result.get("text", "")
        return out
    return None


def _process_emotion_flow(user_message: str) -> Dict[str, Any]:
    """Duygu akƒ±≈üƒ±nƒ± i≈üler - duygu analizi ve sohbet"""
    global chatbot_instance
    if chatbot_instance is None:
        chatbot_instance = EmotionChatbot(client)
    
    result = chatbot_instance.chat(user_message)
    stats = {
        "requests": chatbot_instance.stats["requests"],
        "last_request_at": chatbot_instance.stats["last_request_at"],
    }
    # result: { response: str, first_emoji?: str, second_emoji?: str, request_debug?: str }
    out = {"response": result.get("response", ""), "stats": stats}
    if "first_emoji" in result:
        out["first_emoji"] = result["first_emoji"]
    if "second_emoji" in result:
        out["second_emoji"] = result["second_emoji"]
    if "request_debug" in result:
        out["request_debug"] = result["request_debug"]
    return out


@app.get("/", response_class=HTMLResponse)
def index() -> HTMLResponse:
    """Ana sayfa HTML'ini d√∂nd√ºr√ºr"""
    template_path = Path(__file__).parent / "templates" / "index.html"
    try:
        html = template_path.read_text(encoding="utf-8")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HTML y√ºklenemedi: {e}")
    return HTMLResponse(content=html)


@app.post("/chat")
def chat(payload: Dict[str, Any]) -> Dict[str, Any]:
    """Ana chat endpoint'i - akƒ±≈ü y√∂nlendirmesi yapar"""
    user_message = str(payload.get("message", "")).strip()
    
    # G√ºvenlik kontrolleri
    if not user_message:
        return {"error": "Mesaj bo≈ü olamaz"}
    
    # Mesaj uzunluk kontrol√º
    if not _validate_message_length(user_message):
        return {"error": f"Mesaj √ßok uzun. Maksimum {MAX_MESSAGE_LENGTH} karakter olabilir."}
    
    # Input sanitization
    user_message = _sanitize_input(user_message)
    if user_message == "[G√ºvenlik nedeniyle mesaj filtrelendi]":
        return {"error": "G√ºvenlik nedeniyle mesaj filtrelendi"}
    
    # Token kontrol√º
    estimated_tokens = _estimate_tokens(user_message)
    if estimated_tokens > MAX_TOKENS_PER_REQUEST:
        return {"error": f"√áok fazla token. Maksimum {MAX_TOKENS_PER_REQUEST} token olabilir."}

    try:
        # A≈ûAMA 1: LLM ile akƒ±≈ü y√∂nlendirmesi
        print("[FLOW] A≈ûAMA 1: LLM akƒ±≈ü y√∂nlendirmesi ba≈ülƒ±yor...")
        flow_decision = _get_flow_decision(user_message)
        print(f"[FLOW] LLM akƒ±≈ü kararƒ±: {flow_decision}")

        # A≈ûAMA 2: Se√ßilen akƒ±≈üa g√∂re i≈üleme
        if flow_decision == "RAG":
            print("[FLOW] A≈ûAMA 2: RAG akƒ±≈üƒ± √ßalƒ±≈üƒ±yor...")
            result = _process_rag_flow(user_message)
            if result:
                return result
        elif flow_decision == "ANIMAL":
            print("[FLOW] A≈ûAMA 2: Hayvan akƒ±≈üƒ± √ßalƒ±≈üƒ±yor...")
            result = _process_animal_flow(user_message)
            if result:
                return result
        elif flow_decision == "EMOTION":
            print("[FLOW] A≈ûAMA 2: Duygu akƒ±≈üƒ± √ßalƒ±≈üƒ±yor...")
            result = _process_emotion_flow(user_message)
            if result:
                return result
        
        # Fallback: Varsayƒ±lan duygu akƒ±≈üƒ±
        print("[FLOW] Fallback: Duygu akƒ±≈üƒ± √ßalƒ±≈üƒ±yor...")
        result = _process_emotion_flow(user_message)
        return result

    except HTTPException as e:
        return {"error": e.detail}
    except Exception as e:
        return {"error": str(e)}


# √áalƒ±≈ütƒ±rma:
# uvicorn api_web_chatbot:app --host 0.0.0.0 --port 8000 --reload
